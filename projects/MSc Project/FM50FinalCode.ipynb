{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "ea00cb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import pywt\n",
    "from scipy.special import gamma\n",
    "from scipy import linalg as la\n",
    "from scipy import optimize as opt\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "###################################################################### PARAMETER/DATA LOADING  ########################################################\n",
    "\n",
    "\"\"\"Random Seed\"\"\"\n",
    "seed = 42\n",
    "\n",
    "\n",
    "\"\"\"Data Loading\"\"\"\n",
    "X = np.loadtxt(\"fBMPath2.txt\")\n",
    "BH05 = np.loadtxt(\"BHpath.05.txt\")\n",
    "B05 = np.loadtxt(\"Bpath.05.txt\")\n",
    "W05 = np.loadtxt(\"Wpath.05.txt\")\n",
    "BH10 = np.loadtxt(\"BHpath.10.txt\")\n",
    "B10 = np.loadtxt(\"Bpath.10.txt\")\n",
    "W10 = np.loadtxt(\"Wpath.10.txt\")\n",
    "realized_var3 = np.loadtxt('SPX3yrDailyRealizedVariance1minBins.txt')\n",
    "\n",
    "\n",
    "\"\"\"Simulating Standard fBM Paths \"\"\"\n",
    "scaling_factor = 1000\n",
    "fbm_n = 1024\n",
    "fbm_H_values = [0.05, 0.5, 0.9]\n",
    "T = 1\n",
    "\n",
    "\n",
    "\"\"\"Simulating RSFV \"\"\"\n",
    "H = 0.1\n",
    "S0 = 1\n",
    "V0_1 = 0.1   \n",
    "nu = 1\n",
    "rho = -0.65    \n",
    "\n",
    "\n",
    "\"\"\"MC MLE StdBM Bias\"\"\"\n",
    "m_stdBM = 512\n",
    "n_sim_stdBM = 100\n",
    "n_steps_stdBM = 65536\n",
    "H_stdBM = 0.5\n",
    "p = 95\n",
    "m = [16, 32, 64]\n",
    "\n",
    "\n",
    "\"\"\"RSFV ATM Vol Smile\"\"\"\n",
    "V0_2 = 0.04\n",
    "smile_N = 252        \n",
    "smile_M = 100000\n",
    "\n",
    "\n",
    "\"\"\"Estimator, Models, Colors\"\"\"\n",
    "models = ['Clean fBM', 'Contaminated fBM', 'RFSV', 'fOU']\n",
    "estimators = ['MLE', 'Power Variation', 'Wavelet', 'Log-Log Regression']\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "\n",
    "\"\"\"Models C-fBM, RSFV, fOU Parameters\"\"\"\n",
    "noise_level=0.2\n",
    "jump_intensity=0.05 \n",
    "jump_size=0.25\n",
    "\n",
    "nu_3 = 1.5\n",
    "\n",
    "theta=2.5\n",
    "mu=0\n",
    "sigma=1.0\n",
    "\n",
    "\n",
    "\"\"\"MC Bias + SPX\"\"\"\n",
    "H_vals = (0.1, 0.3, 0.5, 0.7, 0.9)\n",
    "hist_Vals=(0.1, 0.9)\n",
    "T_vals = [5]\n",
    "\n",
    "monte_N = 256\n",
    "monte_M = 50\n",
    "\n",
    "\n",
    "\"\"\"Estimator Convergence\"\"\"\n",
    "conv_H = 0.1\n",
    "conv_T = 1\n",
    "\n",
    "conv_N = [64, 128, 256, 512, 1024, 2048]\n",
    "conv_M = 50\n",
    "\n",
    "\n",
    "\"\"\"Windows\"\"\"\n",
    "monte_window = 32\n",
    "spx_window = 32\n",
    "conv_window = 32\n",
    "\n",
    "\n",
    "\"\"\"Faster Sims\"\"\"\n",
    "#smile_N = 128 \n",
    "#smile_M = 10000\n",
    "\n",
    "#monte_N = 128\n",
    "#monte_M = 25\n",
    "\n",
    "#conv_N = [64, 128, 256, 512] \n",
    "#conv_M = 25\n",
    "\n",
    "###################################################################### Main Functions #################################################################\n",
    "\n",
    "######################################################################  Universal Functions  #\n",
    "\n",
    "def compute_covariance_matrix(H, t):\n",
    "    t1, t2 = np.meshgrid(t, t)\n",
    "    \n",
    "    cov = 0.5 * (t1**(2*H) + t2**(2*H) - np.abs(t1 - t2)**(2*H))\n",
    "    \n",
    "    return cov\n",
    "\n",
    "def generate_fbm(H, n, T): \n",
    "    t = np.arange(1, n+1)/n\n",
    "    \n",
    "    cov = compute_covariance_matrix(H, t)\n",
    "    L = la.cholesky(cov, lower=True)\n",
    "    Z = np.random.normal(0, 1, size=n)\n",
    "    clean_path = np.zeros(n+1)\n",
    "    clean_path[1:] = L @ Z\n",
    "\n",
    "    timeline = np.linspace(0, T, n+1)\n",
    "    clean_scale = clean_path* (T**H)\n",
    "    \n",
    "    return timeline, clean_scale, clean_path\n",
    "    \n",
    "def confidence_interval(true_value, n, arr, p=p):\n",
    "    std = np.std(arr, ddof=1)\n",
    "    mean = np.mean(arr)\n",
    "    \n",
    "    se = std / np.sqrt(n)\n",
    "    z = stats.norm.ppf(1 - (100 - p) / 200.0)\n",
    "    \n",
    "    ci_low = mean - z * se\n",
    "    ci_high = mean + z * se\n",
    "    \n",
    "    bias = mean - true_value\n",
    "\n",
    "    return mean, ci_low, ci_high, bias, std\n",
    "    \n",
    "######################################################################  Other Path Generators  #\n",
    "\n",
    "def generate_c_fbm(H, n, T):\n",
    "    _,_,clean_path = generate_fbm(H, n, T)\n",
    "\n",
    "    vol = np.std(np.diff(clean_path))\n",
    "    microstructure_noise = noise_level * vol * np.random.normal(0, 1, size=n+1)\n",
    "    microstructure_noise[0] = 0\n",
    "\n",
    "    total_jumps = np.random.poisson(jump_intensity * n) \n",
    "    jump_times = np.random.choice(np.arange(1, n+1), size=total_jumps, replace=False)\n",
    "    jump_sizes = jump_size * vol * np.exp(np.random.normal(0, 1, size=total_jumps))\n",
    "    signed_jumps = jump_sizes * np.sign(np.random.normal(0, 1, size=total_jumps))\n",
    "\n",
    "    contaminated_path = clean_path + microstructure_noise\n",
    "    contaminated_path[jump_times] += signed_jumps\n",
    "\n",
    "    contaminated_scale = contaminated_path * (T**H)\n",
    "    \n",
    "    return contaminated_scale\n",
    "\n",
    "def generate_rfsv(H, n, T, S0=S0, V0=V0_2, nu=nu_3, rho=rho):\n",
    "    t, BH, _ = generate_fbm(H, n, T)\n",
    "    dt = T/n\n",
    "    \n",
    "    B = np.zeros(n+1)\n",
    "    W = np.zeros(n+1)\n",
    "    dB = np.random.normal(0, np.sqrt(dt), n)\n",
    "    dW = np.random.normal(0, np.sqrt(dt), n)\n",
    "    B[1:] = np.cumsum(dB)\n",
    "    W[1:] = np.cumsum(dW)\n",
    "    \n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    S, V = simulate_rfsv_path(BH, B, W, S0, V0, nu, rho)\n",
    "    \n",
    "    logV = np.log(V) \n",
    "    logV = logV - np.mean(logV) \n",
    "    \n",
    "    return logV\n",
    "\n",
    "def generate_fou(H, n, T, theta=theta, mu=mu, sigma=sigma):\n",
    "    t, fbm, _ = generate_fbm(H, n, T)\n",
    "    dt = T/n\n",
    "\n",
    "    dfbm = np.diff(fbm)\n",
    "    X = np.zeros(n+1)\n",
    "    X[0] = mu\n",
    "    \n",
    "    for i in range(n):\n",
    "        X[i+1] = X[i] + theta*(mu - X[i])*dt + sigma*dfbm[i]\n",
    "\n",
    "    return X\n",
    "    \n",
    "######################################################################  RSFV Functions  #\n",
    "\n",
    "def simulate_rfsv_path(BH, B, W, S0=S0, V0=V0_1, nu=nu, rho=rho):\n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    n = len(B) - 1\n",
    "    V = V0 * np.exp(nu * BH) \n",
    "    S = np.zeros(n + 1)\n",
    "    \n",
    "    S[0] = S0\n",
    "    \n",
    "    dB = np.diff(B)\n",
    "    dW = np.diff(W)\n",
    "    \n",
    "    for i in range(n):\n",
    "        sqrtV = np.sqrt(V[i])\n",
    "        S[i+1] = S[i] * (1 + sqrtV * (rho * dB[i] + rho_bar * dW[i]))\n",
    "    \n",
    "    return S, V\n",
    "\n",
    "def simulate_paths(n_sim=n_sim_stdBM, n_steps=n_steps_stdBM, block_size=m_stdBM, S0=S0, V0=V0_1, nu=nu, rho=rho):\n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    dt = 1.0 / n_steps  \n",
    "    estimates = []\n",
    "    \n",
    "    for i in range(n_sim):\n",
    "        dB = np.sqrt(dt) * np.random.randn(n_steps)\n",
    "        dW = np.sqrt(dt) * np.random.randn(n_steps)\n",
    "        \n",
    "        B_path = np.concatenate([[0], np.cumsum(dB)])\n",
    "        W_path = np.concatenate([[0], np.cumsum(dW)])\n",
    "        BH_path = B_path.copy()\n",
    "        \n",
    "        S, _ = simulate_rfsv_path(BH_path, B_path, W_path, S0=S0, V0=V0, nu=nu, rho=rho)\n",
    "        \n",
    "        log_rets = np.diff(np.log(S))\n",
    "        \n",
    "        spot_vars = compute_realized_variances(log_rets, block_size)\n",
    "        log_vars = np.log(spot_vars)\n",
    "        log_vars_centered = log_vars - np.mean(log_vars)\n",
    "        \n",
    "        H_est, _ = mle_estimator(log_vars_centered)\n",
    "        estimates.append(H_est)\n",
    "        estimates_array = np.array(estimates)\n",
    "    \n",
    "    return estimates_array\n",
    "\n",
    "def compute_realized_variances(log_returns, block_size):\n",
    "    n = len(log_returns)\n",
    "    dt = T / n\n",
    "    \n",
    "    num_blocks = n // block_size\n",
    "    spot_vars = np.zeros(num_blocks)\n",
    "    \n",
    "    for j in range(num_blocks):\n",
    "        start_idx = j * block_size\n",
    "        end_idx = start_idx + block_size\n",
    "        block = log_returns[start_idx:end_idx]\n",
    "        spot_vars[j] = np.sum(block**2) / (block_size * dt)\n",
    "        \n",
    "    return spot_vars\n",
    "\n",
    "def estimate_params(BH, B, W, m=m, S0=S0, V0=V0_1, nu=nu, rho=rho):\n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    S, V = simulate_rfsv_path(BH, B, W, S0, V0, nu, rho)\n",
    "    \n",
    "    log_returns = np.diff(np.log(S))\n",
    "    dt = 1.0 / (len(B) - 1)\n",
    "    \n",
    "    results = {}\n",
    "    for block_size in m:\n",
    "        spot_vars = compute_realized_variances(log_returns, block_size)\n",
    "        \n",
    "        log_vars = np.log(spot_vars)\n",
    "        log_vars_centered = log_vars - np.mean(log_vars)\n",
    "        \n",
    "        H_est, nu_est = mle_estimator(log_vars_centered)\n",
    "        results[block_size] = (H_est, nu_est)\n",
    "    \n",
    "    return results\n",
    "\n",
    "######################################################################  Smile  #\n",
    "\n",
    "def simulate_rough_bergomi_paths(H=H, N=smile_N, V0=V0_2, nu=nu, rho=rho, T=T):\n",
    "    dt = T / N\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "    t_grid = np.linspace(0, T, N+1)\n",
    "    correction = 0.5 * nu**2 * (t_grid**(2*H))\n",
    "\n",
    "    lags = dt * np.arange(1, N+1)\n",
    "    c_H = np.sqrt(2*H)\n",
    "    kernel_full = c_H * (lags ** (H - 0.5))\n",
    "\n",
    "    L = np.zeros((N+1, N))\n",
    "    for i in range(1, N+1):\n",
    "        L[i, :i] = kernel_full[:i][::-1] \n",
    "        \n",
    "    return L, correction, dt, sqrt_dt\n",
    "\n",
    "def compute_call_prices(L, correction, dt, sqrt_dt, K, M=smile_M, N=smile_N, S0=S0, V0=V0_2,  nu=nu, rho=rho):\n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    call_price_sum = np.zeros(len(K))\n",
    "    \n",
    "    for m in range(M):\n",
    "        dW = np.random.normal(0, sqrt_dt, N)\n",
    "        dB = np.random.normal(0, sqrt_dt, N)\n",
    "        \n",
    "        BH_orig = L @ dB\n",
    "        BH_ant = L @ (-dB)\n",
    "        \n",
    "        V_orig = V0 * np.exp(nu * BH_orig - correction)\n",
    "        V_ant = V0 * np.exp(nu * BH_ant - correction)\n",
    "        \n",
    "        logS = np.log(S0) * np.ones(4)  \n",
    "        \n",
    "        for i in range(N):\n",
    "            sqrtV_orig = np.sqrt(V_orig[i])\n",
    "            sqrtV_ant = np.sqrt(V_ant[i])\n",
    "\n",
    "            dZ1 = rho * dB[i] + rho_bar * dW[i]     \n",
    "            dZ2 = rho * (-dB[i]) + rho_bar * dW[i]    \n",
    "            dZ3 = rho * dB[i] + rho_bar * (-dW[i])   \n",
    "            dZ4 = rho * (-dB[i]) + rho_bar * (-dW[i]) \n",
    "            \n",
    "            logS[0] += -0.5 * V_orig[i] * dt + sqrtV_orig * dZ1\n",
    "            logS[1] += -0.5 * V_ant[i] * dt + sqrtV_ant * dZ2\n",
    "            logS[2] += -0.5 * V_orig[i] * dt + sqrtV_orig * dZ3\n",
    "            logS[3] += -0.5 * V_ant[i] * dt + sqrtV_ant * dZ4\n",
    "        \n",
    "        S_T = np.exp(logS)\n",
    "        \n",
    "        payoffs = np.maximum(S_T - K[:, np.newaxis], 0)\n",
    "        avg_payoffs = np.mean(payoffs, axis=1)\n",
    "        call_price_sum += avg_payoffs\n",
    "        \n",
    "    avg_call = call_price_sum / M\n",
    "    \n",
    "    return avg_call\n",
    "\n",
    "def bs_price_call(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma*np.sqrt(T)\n",
    "    \n",
    "    call = S * stats.norm.cdf(d1) - K * np.exp(-r*T) * stats.norm.cdf(d2)\n",
    "    \n",
    "    return call\n",
    "    \n",
    "def implied_vol_call(price, S, K, T, r, lower=1e-6, upper=10):\n",
    "    f = lambda sig: bs_price_call(S, K, T, r, sig) - price\n",
    "    \n",
    "    return opt.brentq(f, lower, upper)\n",
    "\n",
    "def compute_implied_vols(call_prices, K, S0=S0, T=T, r=0):\n",
    "    implied_vols = []\n",
    "    \n",
    "    for i, k in enumerate(K):\n",
    "        mc_price = call_prices[i]\n",
    "        iv = implied_vol_call(mc_price, S0, k, T, r)\n",
    "        implied_vols.append(iv)\n",
    "        \n",
    "    implied_vol_array = np.array(implied_vols)\n",
    "    \n",
    "    return implied_vol_array\n",
    "\n",
    "######################################################################  Monte Carlo Estimators  #\n",
    "\n",
    "def monte_carlo_bias(H, T, M, n, monte_window, models=models, estimators=estimators):\n",
    "\n",
    "    estimates = {model: {est: [] for est in estimators} for model in models}\n",
    "\n",
    "    for _ in range(M):\n",
    "        _, clean_fbm, _ = generate_fbm(H, n, T)\n",
    "        contaminated_fbm  = generate_c_fbm(H, n, T)\n",
    "        BH_rfsv = generate_rfsv(H, n, T)\n",
    "        fou = generate_fou(H, n, T)\n",
    "\n",
    "        paths = [clean_fbm, contaminated_fbm, BH_rfsv, fou]\n",
    "\n",
    "        for model, path in zip(models, paths):\n",
    "\n",
    "            H_mle = chunk_array_mle(path, monte_window)\n",
    "            estimates[model]['MLE'].append(H_mle)\n",
    "\n",
    "            H_pv = chunk_array_power(path, T, monte_window)\n",
    "            estimates[model]['Power Variation'].append(H_pv)\n",
    "\n",
    "            H_wave = chunk_array_wavelet(path, monte_window)\n",
    "            estimates[model]['Wavelet'].append(H_wave)\n",
    "\n",
    "            H_loglog = chunk_array_loglog(path, monte_window)\n",
    "            estimates[model]['Log-Log Regression'].append(H_loglog)\n",
    "\n",
    "    results = {model: {} for model in models}\n",
    "\n",
    "    for model in models:\n",
    "        for est in estimators:\n",
    "            arr = np.array(estimates[model][est])\n",
    "            arr = arr[~np.isnan(arr)]\n",
    "\n",
    "            mean_hat, ci_low, ci_high, bias_val, std_val  = confidence_interval(H, len(arr), arr, p=95)\n",
    "            _, jb_pvalue = stats.jarque_bera(arr)\n",
    "         \n",
    "\n",
    "            results[model][est] = {\n",
    "                'mean_hat': mean_hat,\n",
    "                'bias': bias_val,\n",
    "                'std': std_val,\n",
    "                'conf_interval': (ci_low, ci_high),\n",
    "                'jb_pvalue': jb_pvalue\n",
    "            }\n",
    "\n",
    "    return results, estimates\n",
    "\n",
    "def collect_monte_carlo_data(H_vals=H_vals, T_vals=T_vals, monte_N=monte_N, monte_M=monte_M, monte_window=monte_window, models=models, estimators=estimators):\n",
    "    all_results = {}\n",
    "    all_raw = {}\n",
    "    \n",
    "    for T in T_vals:\n",
    "        print(f\"\\n\\n=== T = {T} years ===\")\n",
    "        n = int(monte_N * T)\n",
    "\n",
    "        for H in H_vals:\n",
    "            print(f\"\\nH = {H:.1f}\")\n",
    "\n",
    "            results, raw_estimates = monte_carlo_bias(H, T, monte_M, n, monte_window)\n",
    "            all_results[(H, T)] = results\n",
    "            all_raw[(H, T)] = raw_estimates\n",
    "\n",
    "    return all_results, all_raw\n",
    "\n",
    "######################################################################  Estimators  #\n",
    "\n",
    "def prof_neg_loglik(H, X, t):\n",
    "    n = len(X)\n",
    "    S0 = compute_covariance_matrix(H, t)\n",
    "    \n",
    "    X_scaled = X * scaling_factor\n",
    "    S0_scaled = S0 * (scaling_factor ** 2)\n",
    "    \n",
    "    L = la.cholesky(S0_scaled, lower=True)\n",
    "    y = la.solve_triangular(L, X_scaled, lower=True)\n",
    "    \n",
    "    quad = np.dot(y, y)\n",
    "    logdet = 2 * np.sum(np.log(np.diag(L)))\n",
    "    nu = (n / 2) * np.log(quad / n) + 0.5 * logdet\n",
    "    \n",
    "    return nu\n",
    "\n",
    "def mle_estimator(X, year_correction = 1):\n",
    "    n = len(X)\n",
    "    t = np.arange(1, n + 1) / (n/year_correction)\n",
    "    res = opt.minimize_scalar(\n",
    "        prof_neg_loglik,\n",
    "        args=(X, t),\n",
    "        bounds=(0.0001, 0.9999),\n",
    "        method='bounded'\n",
    "    )\n",
    "    H_hat = res.x\n",
    "\n",
    "    S0_hat = compute_covariance_matrix(H_hat, t) * (scaling_factor ** 2)\n",
    "    Lh = la.cholesky(S0_hat, lower=True)\n",
    "    y_h = la.solve_triangular(Lh, X * scaling_factor, lower=True)\n",
    "    nu_hat = np.sqrt(np.dot(y_h, y_h) / n)\n",
    "    \n",
    "    return H_hat, nu_hat\n",
    "\n",
    "def log_log_estimator(X, q=2):\n",
    "    n = len(X)\n",
    "    k_values = [2**j for j in range(int(np.log2(n)))]\n",
    "    \n",
    "    m_q = np.array([np.mean(np.abs(X[k:] - X[:-k])**q) for k in k_values])\n",
    "    log_m = np.log(m_q)\n",
    "    log_deltas = np.log(np.array(k_values))\n",
    "    slope = stats.linregress(log_deltas, log_m)[0]\n",
    "    H_est = slope / q\n",
    "    \n",
    "    return H_est\n",
    "\n",
    "def power_variation_estimator(X, T, q=2):\n",
    "    inc = np.diff(np.asarray(X))\n",
    "    n = len(inc)\n",
    "    dt = T / n  \n",
    "\n",
    "    K_q = 2**(q/2) * gamma((q+1)/2) / np.sqrt(np.pi)\n",
    "    V_q = np.mean(np.abs(inc)**q)\n",
    "\n",
    "    H = (np.log(V_q) - np.log(K_q)) / (q * np.log(dt))\n",
    "    return float(np.clip(H, 1e-4, 0.9999))\n",
    "\n",
    "\n",
    "def wavelet_estimator(X, wavelet='db3'):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    \n",
    "    W = pywt.Wavelet(wavelet)\n",
    "    max_level = pywt.dwt_max_level(len(X), W.dec_len)\n",
    "    \n",
    "    coeffs = pywt.wavedec(X, W, level=max_level)\n",
    "    details = coeffs[1:]  \n",
    "\n",
    "    n_j = np.array([len(d) for d in details], dtype=float)\n",
    "    S2_j = np.array([np.mean(d**2) for d in details], dtype=float)\n",
    "\n",
    "    j = -np.log2(n_j)\n",
    "    y = np.log2(S2_j)\n",
    "    w_j = np.sqrt(n_j)\n",
    "\n",
    "    beta_hat, alpha_hat = np.polyfit(j, y, deg=1, w=w_j)\n",
    "    H_hat = 0.5 * (beta_hat - 1.0)\n",
    "    \n",
    "    return float(np.clip(H_hat, 1e-4, 0.9999))\n",
    "\n",
    "    \n",
    "    \n",
    "######################################################################  Monte Carlo Blocking Functions  #\n",
    "\n",
    "def blocks_with_tail(a, l):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    if a.size < l:\n",
    "        return np.empty((0, l))  \n",
    "    k = a.size // l\n",
    "    starts = list(range(0, max(k-1, 0) * l, l))\n",
    "    starts.append(a.size - l) \n",
    "    return np.stack([a[s:s+l] for s in starts], axis=0)\n",
    "\n",
    "def chunk_array_wavelet(X, block_size):\n",
    "    blocks = blocks_with_tail(X, block_size)\n",
    "    if blocks.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    H_values = []\n",
    "    for i in range(blocks.shape[0]):\n",
    "        b = blocks[i]\n",
    "        H_block = wavelet_estimator(b)\n",
    "        H_values.append(H_block)\n",
    "\n",
    "    H_values = np.array(H_values)\n",
    "    return np.mean(H_values)\n",
    "\n",
    "def chunk_array_mle(X, block_size):\n",
    "    blocks = blocks_with_tail(X, block_size)\n",
    "    if blocks.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    H_values = []\n",
    "    for i in range(blocks.shape[0]):\n",
    "        b = blocks[i]\n",
    "        H_block, _ = mle_estimator(b)\n",
    "        H_values.append(H_block)\n",
    "\n",
    "    H_values = np.array(H_values)\n",
    "    return np.mean(H_values)\n",
    "\n",
    "def chunk_array_power(X, T, block_size):\n",
    "    blocks = blocks_with_tail(X, block_size)\n",
    "    if blocks.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    dt_global = T / (len(X) - 1)\n",
    "    T_block = dt_global * (block_size - 1)\n",
    "\n",
    "    H_values = []\n",
    "    for i in range(blocks.shape[0]):\n",
    "        b = blocks[i]\n",
    "        H_block = power_variation_estimator(b, T_block)\n",
    "        H_values.append(H_block)\n",
    "\n",
    "    H_values = np.array(H_values)\n",
    "    return np.mean(H_values)\n",
    "\n",
    "def chunk_array_loglog(X, block_size):\n",
    "    blocks = blocks_with_tail(X, block_size)\n",
    "    if blocks.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    H_values = []\n",
    "    for i in range(blocks.shape[0]):\n",
    "        b = blocks[i]\n",
    "        H_block = log_log_estimator(b)\n",
    "        H_values.append(H_block)\n",
    "\n",
    "    H_values = np.array(H_values)\n",
    "    return np.mean(H_values)\n",
    "    \n",
    "######################################################################  Plotting Functions  #\n",
    "\n",
    "def plot_fbm_paths(H_values, t_full, T):\n",
    "    n = len(t_full) - 1\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for H in H_values:\n",
    "        _,_,fBm_full = generate_fbm(H, n, T)\n",
    "        plt.plot(t_full, fBm_full, label=f'H={H}')\n",
    "    \n",
    "    plt.title(\"Fractional Brownian Motion Paths (Cholesky Method)\")\n",
    "    plt.xlabel(\"Time (t)\")\n",
    "    plt.ylabel(\"$B_t^H$\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_RFSV_path(BH, B, W, S0=S0, V0=V0_1, nu=nu, rho=rho):\n",
    "    rho_bar = np.sqrt(1 - rho**2)\n",
    "    n = len(BH) - 1\n",
    "    dt = 1.0 / n\n",
    "    tgrid = np.linspace(0, 1, n + 1)\n",
    "\n",
    "    V = V0 * np.exp(nu * BH)\n",
    "    dB = np.diff(B)\n",
    "    dW = np.diff(W)\n",
    "    S = np.empty(n + 1)\n",
    "    S[0] = S0\n",
    "    for i in range(n):\n",
    "        S[i+1] = S[i] + S[i] * np.sqrt(V[i]) * (rho * dB[i] + rho_bar * dW[i])\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(tgrid, V)\n",
    "    plt.title('Variance Process Simulation with RFSV Model', fontsize = 10)\n",
    "    plt.xlabel('Time (t)')\n",
    "    plt.ylabel('$V_t$')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(tgrid, S)\n",
    "    plt.title('Stock Price Simulation with RFSV Model', fontsize = 10)\n",
    "    plt.xlabel('Time (t)')\n",
    "    plt.ylabel('$S_t$')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_smile(x, implied_vols, H=H, nu=nu, V0=V0_2, rho=rho):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, implied_vols, 'bo-', linewidth=2, markersize=6)\n",
    "    plt.title(f'Rough Bergomi Implied Volatility Smile (H={H}, ρ={rho})', fontsize=14)\n",
    "    plt.xlabel('Log-Moneyness (x = log(K/S₀))', fontsize=12)\n",
    "    plt.ylabel('Implied Volatility', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_spx_hurst(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    segments = [r['Segment'] for r in results]\n",
    "    x = np.arange(len(segments))\n",
    "    width = 0.8 / len(estimators)\n",
    "\n",
    "    for i, e in enumerate(estimators):\n",
    "        vals = [r[e] for r in results]\n",
    "        plt.bar(x + width * (i - (len(estimators) - 1) / 2), vals, width, label=e, color=colors[i])\n",
    "\n",
    "    plt.xticks(x, segments)\n",
    "    plt.ylabel('H Estimate')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(0, 0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_line_H(results_dict, H_vals=H_vals, colors=colors, estimators=estimators):\n",
    "    models = list(results_dict[(H_vals[0], T_vals[0])].keys())\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"Model: {model}\")\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.xlabel('True Hurst Exponent (H)', fontsize=12)\n",
    "        plt.ylabel('Estimated H', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.plot(H_vals, H_vals, 'k-', linewidth=3, alpha=0.7, label='True H')\n",
    "        \n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.yticks(np.arange(0.1, 1.0, 0.2))\n",
    "        plt.xticks(H_vals)\n",
    "        \n",
    "        for est_idx, (estimator, color, marker) in enumerate(zip(estimators, colors, markers)):\n",
    "            mean_hats = []\n",
    "            ci_lows = []\n",
    "            ci_highs = []\n",
    "            \n",
    "            for H in H_vals:\n",
    "                res = results_dict[(H, T_vals[0])][model][estimator]\n",
    "                mean_hats.append(res['mean_hat'])\n",
    "                ci_low, ci_high = res['conf_interval']\n",
    "                ci_lows.append(ci_low)\n",
    "                ci_highs.append(ci_high)\n",
    "            \n",
    "            plt.plot(H_vals, mean_hats, '-', color=color, alpha=0.7, label=estimator)\n",
    "            plt.fill_between(H_vals, ci_lows, ci_highs, color=color, alpha=0.2)\n",
    "            plt.scatter(H_vals, mean_hats, marker=marker, s=80, color=color, edgecolor='black', zorder=10)\n",
    "        \n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_boxplots(all_raw, H_vals, T, colors=colors):\n",
    "    nH = len(H_vals)\n",
    "    nE = len(estimators)\n",
    "    group_gap = 1.0\n",
    "    width = 0.6\n",
    "\n",
    "    offsets = np.linspace(-1.2, 1.2, nE) * (width / 1.0)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"Model: {model}\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "        data_list = []\n",
    "        positions = []\n",
    "        for h_idx, H in enumerate(H_vals):\n",
    "            group_center = h_idx * (nE + group_gap)\n",
    "            for e_idx, est in enumerate(estimators):\n",
    "                pos = group_center + offsets[e_idx]\n",
    "                positions.append(pos)\n",
    "\n",
    "                raw = all_raw.get((H, T), {}).get(model, {}).get(est, [])\n",
    "                arr = np.array(raw, dtype=float)\n",
    "                arr = arr[~np.isnan(arr)]\n",
    "                if len(arr) == 0:\n",
    "                    data_list.append(np.array([np.nan]))\n",
    "                else:\n",
    "                    data_list.append(arr)\n",
    "\n",
    "        bp = ax.boxplot(\n",
    "            data_list,\n",
    "            positions=positions,\n",
    "            widths=width,\n",
    "            notch=True,\n",
    "            patch_artist=True,\n",
    "            showmeans=False,\n",
    "            meanprops=dict(marker='D', markeredgecolor='k', markerfacecolor='white', markersize=4),\n",
    "            medianprops=dict(color='black', linewidth=1.5),\n",
    "            flierprops=dict(marker='.', markersize=2.5, alpha=0.6),\n",
    "            manage_ticks=False\n",
    "        )\n",
    "\n",
    "        for k, patch in enumerate(bp['boxes']):\n",
    "            color = colors[k % nE]\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.65)\n",
    "\n",
    "        if 'fliers' in bp:\n",
    "            for fl in bp['fliers']:\n",
    "                try:\n",
    "                    fl.set_marker('.')\n",
    "                    fl.set_markersize(2.5)\n",
    "                    fl.set_alpha(0.6)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        group_centers = [h_idx * (nE + group_gap) for h_idx in range(nH)]\n",
    "        ax.set_xticks(group_centers)\n",
    "        ax.set_xticklabels([f\"{H:.1f}\" for H in H_vals])\n",
    "        ax.set_xlim(- (nE + group_gap), group_centers[-1] + (nE + group_gap))\n",
    "\n",
    "        ax.set_yticks(H_vals)\n",
    "        ax.set_ylim(min(H_vals) - 0.05, max(H_vals) + 0.05)\n",
    "\n",
    "        ax.set_xlabel(\"True H\")\n",
    "        ax.set_ylabel(\"Estimated H\")\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "        proxies = [Patch(facecolor=colors[k], edgecolor='k', alpha=0.6) for k in range(nE)]\n",
    "        ax.legend(proxies, estimators, loc='upper left', fontsize='small')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "def plot_histograms(all_raw, H_vals=hist_Vals, T_val=T_vals[0], colors=colors, estimators=estimators, models=models, bins=20):\n",
    "    target_H = H_vals[:2]\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"Model: {model}\")\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        for ax, H in zip(axes, target_H):\n",
    "            model_data = all_raw.get((H, T_val), {}).get(model, {})\n",
    "\n",
    "            all_chunks = []\n",
    "            for est in estimators:\n",
    "                arr = np.asarray(model_data.get(est, []), dtype=float)\n",
    "                arr = arr[~np.isnan(arr)]\n",
    "                if arr.size > 0:\n",
    "                    all_chunks.append(arr)\n",
    "\n",
    "            if not all_chunks:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "\n",
    "            all_vals = np.concatenate(all_chunks)\n",
    "            xmin, xmax = np.min(all_vals), np.max(all_vals)\n",
    "            if xmin == xmax:\n",
    "                xmin -= 0.05\n",
    "                xmax += 0.05\n",
    "\n",
    "            pad = 0.05 * (xmax - xmin)\n",
    "            x_grid = np.linspace(xmin - pad, xmax + pad, 1000)\n",
    "\n",
    "            local_ymax = 0.0\n",
    "            for est, color in zip(estimators, colors):\n",
    "                clean = np.asarray(model_data.get(est, []), dtype=float)\n",
    "                clean = clean[~np.isnan(clean)]\n",
    "                if clean.size == 0:\n",
    "                    continue\n",
    "\n",
    "                n, _, _ = ax.hist(clean, bins=bins, density=True, alpha=0.35,\n",
    "                                  color=color, edgecolor=\"white\", linewidth=0.8)\n",
    "                local_ymax = max(local_ymax, float(np.max(n)))\n",
    "\n",
    "                mu, sd = np.mean(clean), np.std(clean, ddof=1)\n",
    "                if sd > 0:\n",
    "                    y = stats.norm.pdf(x_grid, mu, sd)\n",
    "                    ax.plot(x_grid, y, color=color, linewidth=2.8, alpha=0.95)\n",
    "                    local_ymax = max(local_ymax, float(np.max(y)))\n",
    "\n",
    "            ax.axvline(H, color=\"black\", linestyle=\"--\", linewidth=1.5, label=f\"True $H$ = {H:g}\")\n",
    "            ax.legend(loc=\"upper left\", frameon=False, fontsize=10, handlelength=2.5)\n",
    "\n",
    "            ax.set_xlim(xmin - pad, xmax + pad)\n",
    "            ax.set_ylim(0, min(local_ymax * 1.10, 150))\n",
    "\n",
    "            ax.set_xlabel(\"Estimated H\", fontsize=12)\n",
    "            ax.set_ylabel(\"Density\", fontsize=12)\n",
    "            ax.grid(True, alpha=0.25)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "def plot_rmse_convergence(results_rmse, conv_H, conv_T, p=p, models=models, estimators=estimators, colors=colors):\n",
    "    z = stats.norm.ppf(1 - (100 - p) / 200.0)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    model_positions = {m: divmod(i, 2) for i, m in enumerate(models)}\n",
    "\n",
    "    for model, (i, j) in model_positions.items():\n",
    "        ax = axs[i, j]\n",
    "        for est, color in zip(estimators, colors):\n",
    "            n_vals = np.array(sorted(results_rmse[model][est].keys()))\n",
    "            rmse_vals = np.array([results_rmse[model][est][n] for n in n_vals])\n",
    "\n",
    "            ln_n = np.log(n_vals)\n",
    "            ln_rmse = np.log(rmse_vals)\n",
    "            X = sm.add_constant(ln_n)\n",
    "            fit = sm.OLS(ln_rmse, X).fit()\n",
    "            slope, se_slope = fit.params[1], fit.bse[1]\n",
    "\n",
    "            ax.loglog(n_vals, rmse_vals, 'o-', color=color)\n",
    "\n",
    "            ci_upper = np.exp(fit.params[0] + z*fit.bse[0]) * n_vals**slope\n",
    "            ci_lower = np.exp(fit.params[0] - z*fit.bse[0]) * n_vals**slope\n",
    "            ax.fill_between(n_vals, ci_lower, ci_upper, color=color, alpha=0.2)\n",
    "\n",
    "        ax.set_title(f\"{model}\")\n",
    "        ax.set_xlabel(\"Sample Size (n)\")\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "        ax.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_bias_variance_decomposition(results_bias2, results_var, conv_H, conv_T, models=models, estimators=estimators, colors=colors):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    model_positions = {m: divmod(i, 2) for i, m in enumerate(models)}\n",
    "    color_map = dict(zip(estimators, colors))\n",
    "\n",
    "    for model, (i, j) in model_positions.items():\n",
    "        ax = axs[i, j]\n",
    "\n",
    "        for est in estimators:\n",
    "            c = color_map[est]\n",
    "            n_vals = np.array(sorted(results_bias2[model][est].keys()))\n",
    "            vals   = np.array([results_bias2[model][est][n] for n in n_vals])\n",
    "            ax.loglog(n_vals, vals, linestyle='-', color=c, linewidth=2)\n",
    "\n",
    "        for est in estimators:\n",
    "            c = color_map[est]\n",
    "            n_vals = np.array(sorted(results_var[model][est].keys()))\n",
    "            vals   = np.array([results_var[model][est][n] for n in n_vals])\n",
    "            ax.loglog(n_vals, vals, linestyle=':', color=c, linewidth=2)\n",
    "\n",
    "        ax.set_title(f\"{model}\")\n",
    "        ax.set_xlabel(\"Sample Size (n)\")\n",
    "        ax.set_ylabel(\"Bias² / Variance\")\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", alpha=0.6)\n",
    "\n",
    "    style_handles = [\n",
    "        Line2D([0], [0], linestyle='-', color='black', lw=3, label='Bias²'),\n",
    "        Line2D([0], [0], linestyle=':', color='black', lw=3, label='Variance'),\n",
    "    ]\n",
    "    est_handles = [\n",
    "        Line2D([0], [0], linestyle='-', color=color_map[e], lw=3, label=e)\n",
    "        for e in estimators\n",
    "    ]\n",
    "    handles = style_handles + est_handles\n",
    "    labels  = [h.get_label() for h in handles]\n",
    "\n",
    "    fig.legend(handles, labels,loc='lower center',ncol=len(handles),frameon=False,bbox_to_anchor=(0.5, 0.02),handlelength=2.8,columnspacing=1.0)\n",
    "    plt.show()\n",
    "\n",
    "######################################################################  tables  #\n",
    "\n",
    "def spx_results(data, window_size, trading_days_per_year=252):\n",
    "    n = len(data)\n",
    "    seg_len = n // 3\n",
    "    segments = {\n",
    "        'First Year': data[:seg_len],\n",
    "        'Second Year': data[seg_len:2*seg_len],\n",
    "        'Third Year': data[2*seg_len:],\n",
    "        'Full Sample': data\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, seg in segments.items():\n",
    "        centered = np.log(seg) - np.mean(np.log(seg))\n",
    "        years = len(seg) / trading_days_per_year\n",
    "\n",
    "        results.append({\n",
    "            'Segment': name,\n",
    "            'Days': len(seg),\n",
    "            'Years': years,\n",
    "            'MLE': chunk_array_mle(centered, window_size),\n",
    "            'Power Variation': chunk_array_power(centered, years, window_size),\n",
    "            'Wavelet': chunk_array_wavelet(centered, window_size),\n",
    "            'Log-Log Regression': chunk_array_loglog(centered, window_size)\n",
    "        })\n",
    "\n",
    "    col1, colw = 20, 20\n",
    "    header = \"Segment\".ljust(col1)\n",
    "    for e in estimators:\n",
    "        header += f\"{e}\".center(colw)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for r in results:\n",
    "        row = str(r[\"Segment\"]).ljust(col1)\n",
    "        for e in estimators:\n",
    "            row += f\"{r[e]:.4f}\".center(colw)\n",
    "        print(row)\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_all_MC_tables(all_results, H_vals=H_vals, T_vals=T_vals, models=models, estimators=estimators):\n",
    "    for T in T_vals:\n",
    "        print(f\"\\n\\n=== T = {T} years ===\")\n",
    "        for H in H_vals:\n",
    "            print(f\"\\nH = {H:.1f}\")\n",
    "            results = all_results[(H, T)]\n",
    "\n",
    "            header = \"Estimator\".ljust(20)\n",
    "            for model in models:\n",
    "                header += f\"{model}\".center(30)\n",
    "            print(header)\n",
    "            print(\"-\" * len(header))\n",
    "\n",
    "            for est in estimators:\n",
    "                row1 = est.ljust(20)\n",
    "                row2 = \"bias\".ljust(20)\n",
    "                row3 = \"std\".ljust(20)\n",
    "                row4 = \"CI\".ljust(20)\n",
    "\n",
    "                for model in models:\n",
    "                    res = results[model][est]\n",
    "                    jb_star = format_jb_star(res['jb_pvalue'])\n",
    "\n",
    "                    row1 += f\"{res['mean_hat']:.4f}{jb_star}\".center(30)\n",
    "                    row2 += f\"{res['bias']:.4f}\".center(30)\n",
    "                    row3 += f\"{res['std']:.4f}\".center(30)\n",
    "\n",
    "                    ci_low, ci_high = res['conf_interval']\n",
    "                    row4 += f\"[{ci_low:.4f}, {ci_high:.4f}]\".center(30)\n",
    "\n",
    "                print(row1)\n",
    "                print(row2)\n",
    "                print(row3)\n",
    "                print(row4)\n",
    "                if est != estimators[-1]:\n",
    "                    print(\"-\" * len(header))\n",
    "            print(\"-\" * len(header))\n",
    "\n",
    "def create_convergence_table(results_rmse, results_jb, alpha_results, n_max, models=models, estimators=estimators):\n",
    "    print(f\"\\n\\n=== Convergence Results Table (n = {n_max}) ===\")\n",
    "    \n",
    "    header = \"Estimator\".ljust(20)\n",
    "    for model in models:\n",
    "        header += f\"{model}\".center(30)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for est in estimators:\n",
    "        row1 = est.ljust(20)\n",
    "        row2 = \"rate (±se)\".ljust(20) \n",
    "\n",
    "        for model in models:\n",
    "            rmse_val  = results_rmse[model][est][n_max]\n",
    "            jb_pvalue = results_jb[model][est][n_max]\n",
    "            jb_star   = format_jb_star(jb_pvalue)\n",
    "            rmse_str  = f\"{rmse_val:.4f}{jb_star}\"\n",
    "\n",
    "            alpha_val, alpha_se = alpha_results[model][est]\n",
    "            alpha_str = f\"{alpha_val:.2f}±{alpha_se:.2f}\"\n",
    "\n",
    "            row1 += rmse_str.center(30)\n",
    "            row2 += alpha_str.center(30)\n",
    "\n",
    "        print(row1)\n",
    "        print(row2)\n",
    "        if est != estimators[-1]:\n",
    "            print(\"-\" * len(header))\n",
    "\n",
    "    print(\"-\" * len(header))\n",
    "    return alpha_results\n",
    "\n",
    "######################################################################  Formating  #\n",
    "\n",
    "def format_jb_star(p):\n",
    "\n",
    "    if np.isnan(p):\n",
    "        return \"\"\n",
    "    if p < 0.001:\n",
    "        return \"***\"\n",
    "    if p < 0.01:\n",
    "        return \"**\"\n",
    "    if p < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "    \n",
    "######################################################################  Outputs  ######################################################################\n",
    "\n",
    "\n",
    "######################################################################  Simulating fBM, Estimating H, Nu w/MLE  #\n",
    "np.random.seed(seed)\n",
    "\n",
    "plot_fbm_paths(fbm_H_values, np.linspace(0, T, fbm_n+1), T)\n",
    "H_hat, nu_hat = mle_estimator(X)\n",
    "    \n",
    "print(f\"MLE H = {H_hat:.10f}\")\n",
    "print(f\"MLE ν = {nu_hat:.10f}\")\n",
    "\n",
    "######################################################################  Simulating RSFV, Estimating H, Nu w/MLE  #\n",
    "\n",
    "res05 = estimate_params(BH05, B05, W05)\n",
    "res10 = estimate_params(BH10, B10, W10)\n",
    "estimates = simulate_paths()\n",
    "\n",
    "mean, ci_low, ci_high, bias, std = confidence_interval(H_stdBM, n_sim_stdBM, estimates, p)\n",
    "\n",
    "print(\"\\nH=0.05 results:\")\n",
    "for m_val, vals in res05.items():\n",
    "        print(f\"m={m_val}: H_hat={vals[0]:.4f}, nu_hat={vals[1]:.4f}, relErr={np.abs(vals[0]-0.05)/0.05:.4f}\")  \n",
    "    \n",
    "print(\"\\nH=0.10 results:\")\n",
    "for m_val, vals in res10.items():\n",
    "        print(f\"m={m_val}: H_hat={vals[0]:.4f}, nu_hat={vals[1]:.4f}, relErr={np.abs(vals[0]-0.1)/0.1:.4f}\")\n",
    "\n",
    "print(\"\\nH=\", H_stdBM, \"(std BM) results:\")\n",
    "print(f\"Mean H_hat: {mean:.4f}\")\n",
    "print(f\"Std Dev H_hat: {std:.4f}\")\n",
    "print(f\"{p}% CI: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "print(f\"Bias: {bias:.4f}\")\n",
    "\n",
    "print(\"\\nBH05 Simulation\")\n",
    "plot_RFSV_path(BH05, B05, W05)\n",
    "    \n",
    "print(\"\\nBH10 Simulation\")\n",
    "plot_RFSV_path(BH10, B10, W10)\n",
    "\n",
    "######################################################################  RFSV Implied Volatility Skew  #\n",
    "    \n",
    "x = np.linspace(-0.5, 0.5, 11)  \n",
    "K = S0 * np.exp(x)  \n",
    "L, correction, dt, sqrt_dt = simulate_rough_bergomi_paths()\n",
    "call_prices = compute_call_prices(L, correction, dt, sqrt_dt, K)\n",
    "implied_vols = compute_implied_vols(call_prices, K)\n",
    "    \n",
    "results = pd.DataFrame({\n",
    "    'Log-Moneyness (x)': x,\n",
    "    'Strike Price (K)': K,\n",
    "    'Call Price': call_prices,\n",
    "    'Implied Volatility': implied_vols\n",
    "})\n",
    "print(\"\\nImplied Volatility Smile Results:\")\n",
    "print(results.round(4))\n",
    "\n",
    "plot_smile(x, implied_vols)\n",
    "\n",
    "######################################################################  Estimating H on spx w/MLE  #\n",
    "\n",
    "realized_var1 = realized_var3[-251:]\n",
    "\n",
    "log_realized_var3 = np.log(realized_var3)\n",
    "centered_log_var3 = log_realized_var3 - np.mean(log_realized_var3)\n",
    "\n",
    "log_realized_var1 = np.log(realized_var1)\n",
    "centered_log_var1 = log_realized_var1 - np.mean(log_realized_var1)\n",
    "\n",
    "H_MLE3spx, nu_MLE3spx  = mle_estimator(centered_log_var3, 3)\n",
    "H_MLE1spx, nu_MLE1spx  = mle_estimator(centered_log_var1)\n",
    "\n",
    "print(\"\\nMLE Estimates w/SPX volatility data\")\n",
    "print(\"3 Years :  \" f\"H = {H_MLE3spx:.4f},  nu = {nu_MLE3spx:.4f} \")\n",
    "print(\"Final Yr:  \"f\"H = {H_MLE1spx:.4f},  nu = {nu_MLE1spx:.4f} \")\n",
    "\n",
    "######################################################################  Estimating H on SPX w/Estimators w/Subwindoes  # \n",
    "\n",
    "print(\"\\nSPX Hurst Exponent: Yearly Segments vs Full Sample\")\n",
    "spx_results = spx_results(realized_var3, spx_window)\n",
    "plot_spx_hurst(spx_results)\n",
    "\n",
    "\n",
    "######################################################################  MC Estimators Bias  #\n",
    "np.random.seed(seed)\n",
    "\n",
    "all_results, all_raw = collect_monte_carlo_data()\n",
    "print_all_MC_tables(all_results)\n",
    "\n",
    "plot_line_H(all_results)\n",
    "plot_boxplots(all_raw, H_vals, T_vals[0])\n",
    "plot_histograms(all_raw)\n",
    "\n",
    "######################################################################  Estimator Convergence  #\n",
    "\n",
    "results_rmse = {model: {est: {} for est in estimators} for model in models}\n",
    "results_bias2 = {model: {est: {} for est in estimators} for model in models}\n",
    "results_var   = {model: {est: {} for est in estimators} for model in models}\n",
    "results_jb    = {model: {est: {} for est in estimators} for model in models}\n",
    "\n",
    "for n in conv_N:\n",
    "    print(f\"\\nRunning n={n} (T={conv_T}, H={conv_H})\")\n",
    "    results, _ = monte_carlo_bias(conv_H, conv_T, conv_M, n, conv_window)\n",
    "    \n",
    "    for model in models:\n",
    "        for est in estimators:\n",
    "            res = results[model][est]\n",
    "            b2  = res['bias']**2\n",
    "            var = res['std']**2\n",
    "            mse = b2 + var\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            results_rmse[model][est][n] = rmse\n",
    "            results_bias2[model][est][n] = b2\n",
    "            results_var[model][est][n]   = var\n",
    "            results_jb[model][est][n]    = res['jb_pvalue']\n",
    "            \n",
    "n_max = max(conv_N)\n",
    "alpha_results = {model: {est: (0.0, 0.0) for est in estimators} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    for est in estimators:\n",
    "        n_vals   = np.array(sorted(results_rmse[model][est].keys()))\n",
    "        rmse_vals = np.array([results_rmse[model][est][n] for n in n_vals])\n",
    "\n",
    "        ln_n    = np.log(n_vals)\n",
    "        ln_rmse = np.log(rmse_vals)\n",
    "        X = sm.add_constant(ln_n)\n",
    "        fit = sm.OLS(ln_rmse, X).fit()\n",
    "\n",
    "        beta = -fit.params[1]\n",
    "        beta_se = fit.bse[1]\n",
    "        alpha_results[model][est] = (beta, beta_se)\n",
    "\n",
    "create_convergence_table(results_rmse, results_jb, alpha_results, n_max)\n",
    "plot_rmse_convergence(results_rmse, conv_H, conv_T)\n",
    "plot_bias_variance_decomposition(results_bias2, results_var, conv_H, conv_T)\n",
    "\n",
    "\n",
    "#end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}